{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":118082,"databundleVersionId":14294892,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":13579687,"sourceType":"datasetVersion","datasetId":8627296},{"sourceId":13673737,"sourceType":"datasetVersion","datasetId":8694435},{"sourceId":13791019,"sourceType":"datasetVersion","datasetId":8779732},{"sourceId":13794636,"sourceType":"datasetVersion","datasetId":8782481}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#loading augmented train data and other files\nimport json\nimport numpy as np\nimport pandas as pd\n\n# Load metric names\nwith open('/kaggle/input/da5401-2025-data-challenge/metric_names.json', 'r') as f:\n    metric_names = json.load(f)\n\n# --- FIX 1: Create the missing metric_map ---\nmetric_map = {name: i for i, name in enumerate(metric_names)}\nprint(\"Created metric_map dictionary.\")\n\n# Load metric embeddings\nmetric_embeddings = np.load('/kaggle/input/da5401-2025-data-challenge/metric_name_embeddings.npy')\n\n# Load train data\naug_train_df = pd.read_csv('/kaggle/input/final-aug/augmented_train_dataset_400 (1).csv')\n\n# --- FIX 2: Copy aug_train_df to train_df before using it ---\ntrain_df = aug_train_df.copy()\nprint(\"Copied aug_train_df to train_df.\")\n\n# Load test data\nwith open('/kaggle/input/da5401-2025-data-challenge/test_data.json', 'r') as f:\n    test_data = json.load(f)\ntest_df = pd.DataFrame(test_data)\n\n# Check basic info and missing data\nprint(\"Train dataset info:\")\nprint(train_df.info())\nprint(\"\\nMissing values in train data:\")\nprint(train_df.isnull().sum())\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"Test dataset info:\")\nprint(test_df.info())\nprint(\"\\nMissing values in test data:\")\nprint(test_df.isnull().sum())\n\n# Normalize text columns (lowercase) and handle missing values\nfor col in ['user_prompt', 'system_prompt', 'response']:\n    train_df[col] = train_df[col].fillna('').str.lower()\n    test_df[col] = test_df[col].fillna('').str.lower()\n\n# Convert score to numeric type (float)\ntrain_df['score'] = pd.to_numeric(train_df['score'], errors='coerce')\n\n# Check for any remaining missing scores\nprint(\"\\n\" + \"=\"*50)\nprint(f\"Missing scores after conversion: {train_df['score'].isnull().sum()}\")\n\nprint(f\"Full training set size: {len(train_df)}\")\nprint(f\"Test set size: {len(test_df)}\")\nprint(f\"Number of unique metrics: {len(metric_names)}\")\nprint(f\"Metric embeddings shape: {metric_embeddings.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T17:12:12.252268Z","iopub.execute_input":"2025-11-19T17:12:12.252969Z","iopub.status.idle":"2025-11-19T17:12:13.096337Z","shell.execute_reply.started":"2025-11-19T17:12:12.252943Z","shell.execute_reply":"2025-11-19T17:12:13.095575Z"}},"outputs":[{"name":"stdout","text":"Created metric_map dictionary.\nCopied aug_train_df to train_df.\nTrain dataset info:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 7806 entries, 0 to 7805\nData columns (total 5 columns):\n #   Column         Non-Null Count  Dtype \n---  ------         --------------  ----- \n 0   metric_name    7806 non-null   object\n 1   score          7806 non-null   int64 \n 2   user_prompt    7806 non-null   object\n 3   response       7805 non-null   object\n 4   system_prompt  6126 non-null   object\ndtypes: int64(1), object(4)\nmemory usage: 305.1+ KB\nNone\n\nMissing values in train data:\nmetric_name         0\nscore               0\nuser_prompt         0\nresponse            1\nsystem_prompt    1680\ndtype: int64\n\n==================================================\nTest dataset info:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3638 entries, 0 to 3637\nData columns (total 4 columns):\n #   Column         Non-Null Count  Dtype \n---  ------         --------------  ----- \n 0   metric_name    3638 non-null   object\n 1   user_prompt    3638 non-null   object\n 2   response       3637 non-null   object\n 3   system_prompt  2532 non-null   object\ndtypes: object(4)\nmemory usage: 113.8+ KB\nNone\n\nMissing values in test data:\nmetric_name         0\nuser_prompt         0\nresponse            1\nsystem_prompt    1106\ndtype: int64\n\n==================================================\nMissing scores after conversion: 0\nFull training set size: 7806\nTest set size: 3638\nNumber of unique metrics: 145\nMetric embeddings shape: (145, 768)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"len(aug_train_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T12:39:29.703157Z","iopub.execute_input":"2025-11-19T12:39:29.703698Z","iopub.status.idle":"2025-11-19T12:39:29.708225Z","shell.execute_reply.started":"2025-11-19T12:39:29.703678Z","shell.execute_reply":"2025-11-19T12:39:29.707639Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"7798"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"!pip install --upgrade transformers sentence-transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T11:27:21.390984Z","iopub.execute_input":"2025-11-19T11:27:21.391293Z","iopub.status.idle":"2025-11-19T11:29:01.618458Z","shell.execute_reply.started":"2025-11-19T11:27:21.391267Z","shell.execute_reply":"2025-11-19T11:29:01.617510Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\nCollecting transformers\n  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\nCollecting sentence-transformers\n  Downloading sentence_transformers-5.1.2-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.20.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.36.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.11.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\nCollecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\nRequirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.15.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.10.5)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hDownloading sentence_transformers-5.1.2-py3-none-any.whl (488 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.0/488.0 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, tokenizers, nvidia-cusolver-cu12, transformers, sentence-transformers\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.2\n    Uninstalling tokenizers-0.21.2:\n      Successfully uninstalled tokenizers-0.21.2\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.53.3\n    Uninstalling transformers-4.53.3:\n      Successfully uninstalled transformers-4.53.3\n  Attempting uninstall: sentence-transformers\n    Found existing installation: sentence-transformers 4.1.0\n    Uninstalling sentence-transformers-4.1.0:\n      Successfully uninstalled sentence-transformers-4.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 sentence-transformers-5.1.2 tokenizers-0.22.1 transformers-4.57.1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from huggingface_hub import login\nimport os\nfrom kaggle_secrets import UserSecretsClient\n\n# Log in to Hugging Face\ntry:\n    user_secrets = UserSecretsClient()\n    hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n    login(token=hf_token)\n    print(\"Hugging Face login successful.\")\nexcept:\n    print(\"Hugging Face login failed. Make sure HF_TOKEN is set as a Kaggle Secret.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T11:30:49.925162Z","iopub.execute_input":"2025-11-19T11:30:49.925853Z","iopub.status.idle":"2025-11-19T11:30:50.054804Z","shell.execute_reply.started":"2025-11-19T11:30:49.925829Z","shell.execute_reply":"2025-11-19T11:30:50.054278Z"}},"outputs":[{"name":"stdout","text":"Hugging Face login successful.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Final Safety Filter\n# Drop rows where prompt or response is less than 15 characters (likely noise/fragments)\nprint(len(aug_train_df))\naug_train_df = aug_train_df[aug_train_df['user_prompt'].str.len() > 15]\naug_train_df = aug_train_df[aug_train_df['response'].str.len() > 15]\n\nprint(f\"Final dataset size: {len(aug_train_df)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T11:30:52.827668Z","iopub.execute_input":"2025-11-19T11:30:52.827932Z","iopub.status.idle":"2025-11-19T11:30:52.843528Z","shell.execute_reply.started":"2025-11-19T11:30:52.827912Z","shell.execute_reply":"2025-11-19T11:30:52.842802Z"}},"outputs":[{"name":"stdout","text":"7806\nFinal dataset size: 7798\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"#generating embeddings\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score, mean_squared_error\nfrom sklearn.linear_model import Lasso\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sentence_transformers import SentenceTransformer\nimport torch \nimport warnings\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings('ignore')\n\n# --- 1. Load Prerequisite Data (Assumed to be in memory) ---\n# Make sure these variables are loaded in your environment first:\n# aug_train_df: pd.DataFrame (Your augmented training data)\n# test_df: pd.DataFrame (The original test data)\n# metric_map: dict (Mapping from metric_name to index)\n# metric_embeddings: np.array (Embeddings for all metrics)\n\n# --- 2. Initialize Your Chosen Encoder (FIXED) ---\nprint(\"Loading multilingual-e5-large encoder...\")\n# Explicitly assign to the first available GPU. This is the robust method.\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Assigning encoder to device: {device}\")\nencoder = SentenceTransformer(\"intfloat/multilingual-e5-large\", device=device)\nprint(\"Encoder loaded.\")\n\n# --- 3. Create Copies and Clean Data ---\nprint(\"Cleaning data...\")\ntrain_df = aug_train_df.copy()\ntest_df_proc = test_df.copy()\n\n# Clean train_df\nfor col in ['user_prompt', 'response']:\n    train_df[col] = train_df[col].fillna('').str.lower()\ntrain_df['score'] = pd.to_numeric(train_df['score'], errors='coerce')\n\n# Clean test_df_proc\nfor col in ['user_prompt', 'response']:\n    test_df_proc[col] = test_df_proc[col].fillna('').str.lower()\n\n# --- 4. Define Text Function & Get Full Text ---\ndef get_full_text_no_system(row):\n    \"\"\"Combines prompt/response, *totally dropping* system_prompt.\"\"\"\n    return f\"user: {row['user_prompt']} \\nai: {row['response']}\"\n\nprint(\"Combining text fields...\")\ntrain_df['full_text'] = train_df.apply(get_full_text_no_system, axis=1)\ntest_df_proc['full_text'] = test_df_proc.apply(get_full_text_no_system, axis=1)\n\n# --- 5. Generate and Save Text Embeddings ---\nprint(\"Encoding augmented training text (using GPU)...\")\n# Pass the texts as a list\nE_pr_train_aug = encoder.encode(train_df['full_text'].tolist(), show_progress_bar=True)\nnp.save(\"E_pr_train_aug.npy\", E_pr_train_aug)\nprint(\"Saved E_pr_train_aug.npy\")\n\nprint(\"Encoding test text (using GPU)...\")\nE_pr_test_new = encoder.encode(test_df_proc['full_text'].tolist(), show_progress_bar=True)\nnp.save(\"E_pr_test_new.npy\", E_pr_test_new)\nprint(\"Saved E_pr_test_new.npy\")\n\n# --- 6. Generate and Save Metric Embeddings ---\nprint(\"Mapping metric embeddings...\")\ntrain_metric_indices = train_df['metric_name'].map(metric_map).values\nE_met_train_aug = metric_embeddings[train_metric_indices]\nnp.save(\"E_met_train_aug.npy\", E_met_train_aug)\nprint(\"Saved E_met_train_aug.npy\")\n\ntest_metric_indices = test_df_proc['metric_name'].map(metric_map).values\nE_met_test_new = metric_embeddings[test_metric_indices]\nnp.save(\"E_met_test_new.npy\", E_met_test_new)\nprint(\"Saved E_met_test_new.npy\")\n\n# --- 7. Define Target Variable ---\ny_train_float = train_df['score'].values\n\nprint(\"\\n--- Data Preparation Complete ---\")\nprint(f\"E_pr_train_aug shape: {E_pr_train_aug.shape}\")\nprint(f\"E_met_train_aug shape: {E_met_train_aug.shape}\")\nprint(f\"E_pr_test_new shape: {E_pr_test_new.shape}\")\nprint(f\"E_met_test_new shape: {E_met_test_new.shape}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-19T11:31:27.996964Z","iopub.execute_input":"2025-11-19T11:31:27.997509Z","iopub.status.idle":"2025-11-19T11:45:30.865319Z","shell.execute_reply.started":"2025-11-19T11:31:27.997482Z","shell.execute_reply":"2025-11-19T11:45:30.864574Z"}},"outputs":[{"name":"stderr","text":"2025-11-19 11:31:48.269873: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763551908.694208      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763551908.820375      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"Loading multilingual-e5-large encoder...\nAssigning encoder to device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1e279a4574045f394903260929be5d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1fdb8b0ced14f57bad7a8b7c6220bb6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b38de96017be441fb73220bdb838cff4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/690 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c3748ba3e704e1e89fd8e11207a6173"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c72c79689d14bf9bb76ae6a67329fb6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/418 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80fbefeb3dca4e6893b291465d491980"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0ce2c1f70a242d1a91ba983fdd0bd19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d23a6c00b7e0499093e9d8cba14e084e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abe58e95251a4776a56c68396545ee4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/201 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"395625ffc31d4438b5eb95e9424427e6"}},"metadata":{}},{"name":"stdout","text":"Encoder loaded.\nCleaning data...\nCombining text fields...\nEncoding augmented training text (using GPU)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/244 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"108b596b7f6e48dc83d6ce47a1cbb5ba"}},"metadata":{}},{"name":"stdout","text":"Saved E_pr_train_aug.npy\nEncoding test text (using GPU)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/114 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9eb5016a0154a7893dad2d323a143ca"}},"metadata":{}},{"name":"stdout","text":"Saved E_pr_test_new.npy\nMapping metric embeddings...\nSaved E_met_train_aug.npy\nSaved E_met_test_new.npy\n\n--- Data Preparation Complete ---\nE_pr_train_aug shape: (7798, 1024)\nE_met_train_aug shape: (7798, 768)\nE_pr_test_new shape: (3638, 1024)\nE_met_test_new shape: (3638, 768)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"#Model 1 - single layer MLP\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\n\n# Suppress warnings\nwarnings.filterwarnings('ignore')\n\n\nprint(\"--- Starting Strategy: Shallow 'Bottleneck' MLP ---\")\n\n# --- 1. Load Embeddings ---\nprint(\"Loading pre-computed embeddings...\")\ntry:\n    E_pr_train_aug = np.load(\"E_pr_train_aug.npy\")\n    E_pr_test_new = np.load(\"E_pr_test_new.npy\")\n    E_met_train_aug = np.load(\"E_met_train_aug.npy\")\n    E_met_test_new = np.load(\"E_met_test_new.npy\")\nexcept FileNotFoundError as e:\n    print(f\"Error: {e}\")\n    raise\n\n# --- 2. Create & Scale Features ---\nprint(\"Creating full concatenated features (1792 dims)...\")\nX_train_full = np.hstack([E_pr_train_aug, E_met_train_aug])\nX_test_full = np.hstack([E_pr_test_new, E_met_test_new])\n\n# Neural Networks require scaling\nscaler = StandardScaler()\nX_train_full = scaler.fit_transform(X_train_full)\nX_test_full = scaler.transform(X_test_full)\n\n# --- 3. Define the Shallow Model ---\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nINPUT_DIM = X_train_full.shape[1] # 1792\n\nclass ShallowBottleneckMLP(nn.Module):\n    def __init__(self):\n        super(ShallowBottleneckMLP, self).__init__()\n        # The \"Bottleneck\": Compress 1792 dimensions down to just 16\n        self.layer1 = nn.Linear(INPUT_DIM, 16) \n        self.bn1 = nn.BatchNorm1d(16)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.5) # High dropout\n        self.output = nn.Linear(16, 1) # Predict score\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        return self.output(x)\n\n# --- 4. Setup Sampling Weights (Inverse Frequency) ---\n# Calculate weights for the sampler so low scores are picked more often\nscore_counts = train_df['score'].value_counts().sort_index()\nclass_weights = score_counts / score_counts\n# Map weights to every sample in the training set\nsample_weights = train_df['score'].map(class_weights).values\nsample_weights = torch.DoubleTensor(sample_weights)\n\n# --- 5. K-Fold Training ---\nN_SPLITS = 10\nkf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\nBATCH_SIZE = 256\nEPOCHS = 15\nLR = 0.001\n\noof_predictions = np.zeros(X_train_full.shape[0])\ntest_predictions = np.zeros(X_test_full.shape[0])\n\n# Convert test data to tensor once\nX_test_tensor = torch.tensor(X_test_full, dtype=torch.float32).to(DEVICE)\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_train_full, y_train_float)):\n    print(f\"--- Fold {fold+1}/{N_SPLITS} ---\")\n    \n    # Prepare Data\n    X_tr = torch.tensor(X_train_full[train_idx], dtype=torch.float32)\n    y_tr = torch.tensor(y_train_float[train_idx], dtype=torch.float32).view(-1, 1)\n    X_val = torch.tensor(X_train_full[val_idx], dtype=torch.float32).to(DEVICE)\n    y_val = torch.tensor(y_train_float[val_idx], dtype=torch.float32).view(-1, 1).to(DEVICE)\n    \n    # Create Sampler for THIS fold\n    fold_weights = sample_weights[train_idx]\n    sampler = WeightedRandomSampler(fold_weights, len(fold_weights))\n    \n    train_ds = TensorDataset(X_tr, y_tr)\n    # Use sampler in DataLoader (shuffle must be False when using sampler)\n    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler)\n    \n    # Init Model\n    model = ShallowBottleneckMLP().to(DEVICE)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01) # L2 reg\n    criterion = nn.MSELoss()\n    \n    # Training Loop\n    model.train()\n    for epoch in range(EPOCHS):\n        for X_batch, y_batch in train_loader:\n            X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n            \n            optimizer.zero_grad()\n            preds = model(X_batch)\n            loss = criterion(preds, y_batch)\n            loss.backward()\n            optimizer.step()\n            \n    # Validate\n    model.eval()\n    with torch.no_grad():\n        val_preds = model(X_val).cpu().numpy()\n        oof_predictions[val_idx] = val_preds.flatten()\n        \n        val_rmse = np.sqrt(mean_squared_error(y_val.cpu().numpy(), val_preds))\n        print(f\"Fold {fold+1} Val RMSE: {val_rmse:.4f}\")\n        \n        # Predict on Test\n        test_preds = model(X_test_tensor).cpu().numpy()\n        test_predictions += test_preds.flatten() / N_SPLITS\n\nprint(\"\\n--- Shallow MLP Training Complete ---\")\n\n# --- 6. Post-Processing & Submission ---\noof_rmse_final = np.sqrt(mean_squared_error(y_train_float, oof_predictions))\nprint(f\"OOF RMSE (Shallow MLP): {oof_rmse_final:.4f}\")\n\n# Clip and floor predictions\ntest_predictions = np.clip(test_predictions, 0, 10)\nfinal_predictions = np.floor(test_predictions)\nfinal_predictions_clipped = np.clip(final_predictions, 0, 10)\n\nsubmission_df = pd.DataFrame({\n    'ID': test_df.index + 1,\n    'score': final_predictions_clipped\n})\n\nsubmission_df['score'] = submission_df['score'].astype(float)\nsubmission_df.to_csv(\"submission_shallow_mlp.csv\", index=False) \n\nprint(\"submission_shallow_mlp.csv created successfully!\")\nprint(submission_df.head())\nprint(\"\\nPrediction value counts:\")\nprint(submission_df['score'].value_counts().sort_index())\nprint(submission_df['score'].mean())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T13:03:24.604276Z","iopub.execute_input":"2025-11-19T13:03:24.605039Z","iopub.status.idle":"2025-11-19T13:03:41.974723Z","shell.execute_reply.started":"2025-11-19T13:03:24.605018Z","shell.execute_reply":"2025-11-19T13:03:41.974068Z"}},"outputs":[{"name":"stdout","text":"--- Starting Strategy: Shallow 'Bottleneck' MLP ---\nLoading pre-computed embeddings...\nCreating full concatenated features (1792 dims)...\n--- Fold 1/10 ---\nFold 1 Val RMSE: 3.1567\n--- Fold 2/10 ---\nFold 2 Val RMSE: 2.3293\n--- Fold 3/10 ---\nFold 3 Val RMSE: 2.8694\n--- Fold 4/10 ---\nFold 4 Val RMSE: 2.5451\n--- Fold 5/10 ---\nFold 5 Val RMSE: 2.7110\n--- Fold 6/10 ---\nFold 6 Val RMSE: 2.5177\n--- Fold 7/10 ---\nFold 7 Val RMSE: 2.2596\n--- Fold 8/10 ---\nFold 8 Val RMSE: 2.6834\n--- Fold 9/10 ---\nFold 9 Val RMSE: 3.0593\n--- Fold 10/10 ---\nFold 10 Val RMSE: 2.7143\n\n--- Shallow MLP Training Complete ---\nOOF RMSE (Shallow MLP): 2.6985\nsubmission_shallow_mlp.csv created successfully!\n   ID  score\n0   1    5.0\n1   2    6.0\n2   3    7.0\n3   4    5.0\n4   5    1.0\n\nPrediction value counts:\nscore\n0.0       2\n1.0      10\n2.0      24\n3.0      70\n4.0     249\n5.0     708\n6.0    1222\n7.0    1017\n8.0     316\n9.0      20\nName: count, dtype: int64\n6.037108301264431\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"#model 2 2-layer MLP\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\n\n# Suppress warnings\nwarnings.filterwarnings('ignore')\n\n# --- !! Prerequisite !! ---\n# Assumes .npy files are saved and 'y_train_float', 'test_df', 'train_df' are loaded.\n\nprint(\"--- Starting Strategy: Wider MLP (512 Hidden Neurons) ---\")\n\n# --- 1. Load Embeddings ---\nprint(\"Loading pre-computed embeddings...\")\ntry:\n    E_pr_train_aug = np.load(\"E_pr_train_aug.npy\")\n    E_pr_test_new = np.load(\"E_pr_test_new.npy\")\n    E_met_train_aug = np.load(\"E_met_train_aug.npy\")\n    E_met_test_new = np.load(\"E_met_test_new.npy\")\nexcept FileNotFoundError as e:\n    print(f\"Error: {e}\")\n    raise\n\n# --- 2. Create & Scale Features ---\nprint(\"Creating full concatenated features (1792 dims)...\")\nX_train_full = np.hstack([E_pr_train_aug, E_met_train_aug])\nX_test_full = np.hstack([E_pr_test_new, E_met_test_new])\n\n# Neural Networks require scaling\nscaler = StandardScaler()\nX_train_full = scaler.fit_transform(X_train_full)\nX_test_full = scaler.transform(X_test_full)\n\n# --- 3. Define the Wider Model ---\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nINPUT_DIM = X_train_full.shape[1] # 1792\n\nclass WiderMLP(nn.Module):\n    def __init__(self):\n        super(WiderMLP, self).__init__()\n        # WIDER HIDDEN LAYER: \n        # Increased from 16 to 512 to capture more complex non-linear patterns\n        self.layer1 = nn.Linear(INPUT_DIM, 20) \n        \n        # BatchNorm must match the layer output size (512)\n        self.bn1 = nn.BatchNorm1d(20)\n        \n        # ReLU introduces the non-linearity\n        self.relu = nn.ReLU()\n        \n        # Dropout helps prevent overfitting now that the model has high capacity\n        self.dropout = nn.Dropout(0.5) \n        \n        # Output layer: Takes 512 inputs and predicts 1 score\n        self.output = nn.Linear(20, 1) \n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        return self.output(x)\n\n# --- 4. Setup Sampling Weights (Inverse Frequency) ---\nscore_counts = train_df['score'].value_counts().sort_index()\nclass_weights = 1 / score_counts\nsample_weights = train_df['score'].map(class_weights).values\nsample_weights = torch.DoubleTensor(sample_weights)\n\n# --- 5. K-Fold Training ---\nN_SPLITS = 10\nkf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\nBATCH_SIZE = 256\nEPOCHS = 30\nLR = 0.0008\n\noof_predictions = np.zeros(X_train_full.shape[0])\ntest_predictions = np.zeros(X_test_full.shape[0])\n\n# Convert test data to tensor once\nX_test_tensor = torch.tensor(X_test_full, dtype=torch.float32).to(DEVICE)\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_train_full, y_train_float)):\n    print(f\"--- Fold {fold+1}/{N_SPLITS} ---\")\n    \n    # Prepare Data\n    X_tr = torch.tensor(X_train_full[train_idx], dtype=torch.float32)\n    y_tr = torch.tensor(y_train_float[train_idx], dtype=torch.float32).view(-1, 1)\n    X_val = torch.tensor(X_train_full[val_idx], dtype=torch.float32).to(DEVICE)\n    y_val = torch.tensor(y_train_float[val_idx], dtype=torch.float32).view(-1, 1).to(DEVICE)\n    \n    # Create Sampler for THIS fold\n    fold_weights = sample_weights[train_idx]\n    sampler = WeightedRandomSampler(fold_weights, len(fold_weights))\n    \n    train_ds = TensorDataset(X_tr, y_tr)\n    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler)\n    \n    # Init Model (Using the new WiderMLP)\n    model = WiderMLP().to(DEVICE)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n    criterion = nn.HuberLoss(delta=1.0)\n    \n    # Training Loop\n    model.train()\n    for epoch in range(EPOCHS):\n        for X_batch, y_batch in train_loader:\n            X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n            \n            optimizer.zero_grad()\n            preds = model(X_batch)\n            loss = criterion(preds, y_batch)\n            loss.backward()\n            optimizer.step()\n            \n    # Validate\n    model.eval()\n    with torch.no_grad():\n        val_preds = model(X_val).cpu().numpy()\n        oof_predictions[val_idx] = val_preds.flatten()\n        \n        val_rmse = np.sqrt(mean_squared_error(y_val.cpu().numpy(), val_preds))\n        print(f\"Fold {fold+1} Val RMSE: {val_rmse:.4f}\")\n        \n        # Predict on Test\n        test_preds = model(X_test_tensor).cpu().numpy()\n        test_predictions += test_preds.flatten() / N_SPLITS\n\nprint(\"\\n--- Wider MLP Training Complete ---\")\n\n# --- 6. Post-Processing & Submission ---\noof_rmse_final = np.sqrt(mean_squared_error(y_train_float, oof_predictions))\nprint(f\"OOF RMSE (Wider MLP): {oof_rmse_final:.4f}\")\n\n# Clip and floor predictions\ntest_predictions = np.clip(test_predictions, 0, 10)\nfinal_predictions = np.floor(test_predictions)\nfinal_predictions_clipped = np.clip(final_predictions, 0, 10)\n\nsubmission_df = pd.DataFrame({\n    'ID': test_df.index + 1,\n    'score': final_predictions_clipped\n})\n\nsubmission_df['score'] = submission_df['score'].astype(float)\nsubmission_df.to_csv(\"submission_wider_mlp.csv\", index=False) \n\nprint(\"submission_wider_mlp.csv created successfully!\")\nprint(submission_df.head())\nprint(\"\\nPrediction value counts:\")\nprint(submission_df['score'].value_counts().sort_index())\nprint(submission_df['score'].mean())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T14:11:53.761948Z","iopub.execute_input":"2025-11-19T14:11:53.762702Z","iopub.status.idle":"2025-11-19T14:12:27.308266Z","shell.execute_reply.started":"2025-11-19T14:11:53.762677Z","shell.execute_reply":"2025-11-19T14:12:27.307648Z"}},"outputs":[{"name":"stdout","text":"--- Starting Strategy: Wider MLP (512 Hidden Neurons) ---\nLoading pre-computed embeddings...\nCreating full concatenated features (1792 dims)...\n--- Fold 1/10 ---\nFold 1 Val RMSE: 1.6184\n--- Fold 2/10 ---\nFold 2 Val RMSE: 1.7397\n--- Fold 3/10 ---\nFold 3 Val RMSE: 1.6999\n--- Fold 4/10 ---\nFold 4 Val RMSE: 1.7115\n--- Fold 5/10 ---\nFold 5 Val RMSE: 1.7368\n--- Fold 6/10 ---\nFold 6 Val RMSE: 1.6511\n--- Fold 7/10 ---\nFold 7 Val RMSE: 1.7004\n--- Fold 8/10 ---\nFold 8 Val RMSE: 1.5639\n--- Fold 9/10 ---\nFold 9 Val RMSE: 1.6319\n--- Fold 10/10 ---\nFold 10 Val RMSE: 1.5535\n\n--- Wider MLP Training Complete ---\nOOF RMSE (Wider MLP): 1.6620\nsubmission_wider_mlp.csv created successfully!\n   ID  score\n0   1    7.0\n1   2    7.0\n2   3    8.0\n3   4    7.0\n4   5    3.0\n\nPrediction value counts:\nscore\n1.0        6\n2.0       15\n3.0       52\n4.0       83\n5.0      208\n6.0      486\n7.0     1040\n8.0     1076\n9.0      521\n10.0     151\nName: count, dtype: int64\n7.30263881253436\n","output_type":"stream"}],"execution_count":76},{"cell_type":"code","source":"#Model 3 - MLP regressor\nimport numpy as np\nimport pandas as pd\nimport warnings\n\n# Scikit-Learn Imports\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\n# Suppress warnings\nwarnings.filterwarnings('ignore')\n\ndef main():\n    print(\"--- Starting Strategy: Log-Target MLP with PCA (Sklearn) ---\")\n\n    # --- 1. Load Pre-computed Embeddings ---\n    print(\"Loading .npy files...\")\n    try:\n        E_pr_train_aug = np.load(\"E_pr_train_aug.npy\")\n        E_pr_test_new = np.load(\"E_pr_test_new.npy\")\n        E_met_train_aug = np.load(\"E_met_train_aug.npy\")\n        E_met_test_new = np.load(\"E_met_test_new.npy\")\n    except FileNotFoundError as e:\n        print(f\"Error loading files: {e}\")\n        return\n\n    # Concatenate features (1792 dims)\n    X_train_arr = np.hstack([E_pr_train_aug, E_met_train_aug])\n    X_test_arr  = np.hstack([E_pr_test_new, E_met_test_new])\n\n    print(f\"Feature Matrix Shape: {X_train_arr.shape}\")\n\n    # Convert to DataFrame to satisfy Pipeline column selection\n    # We generate dummy column names: feat_0, feat_1, ...\n    feat_cols = [f\"feat_{i}\" for i in range(X_train_arr.shape[1])]\n    X = pd.DataFrame(X_train_arr, columns=feat_cols)\n    X_test = pd.DataFrame(X_test_arr, columns=feat_cols)\n\n    # --- 2. Target Variable Setup (Log Transformation) ---\n    # Assuming 'y_train_float' is in the global scope\n    if 'y_train_float' not in globals():\n        print(\"Error: 'y_train_float' not found in global scope.\")\n        return\n    \n    y_raw = y_train_float \n    y = np.log1p(y_raw) # Log transformation - KEY IMPROVEMENT for this strategy\n\n    print(f\"Target range (raw): {y_raw.min():.2f} - {y_raw.max():.2f}\")\n    print(f\"Target range (log): {y.min():.4f} - {y.max():.4f}\")\n\n    # --- 3. Define Pipeline ---\n    print(\"🏗️  Step 3: Building model pipeline...\")\n    numeric_features = [c for c in X.columns]\n    \n    # Preprocessing: Scale -> PCA (256 components)\n    preprocess_pipe = ColumnTransformer([\n        (\"scaler\", StandardScaler(), numeric_features),\n        (\"pca\", PCA(n_components=256, random_state=42), numeric_features)\n    ], remainder=\"drop\")\n\n    # MLP Regressor - tuned hyperparameters from your snippet\n    mlp = MLPRegressor(\n        hidden_layer_sizes=(256, 128),\n        activation=\"tanh\",\n        solver=\"adam\",\n        learning_rate_init=0.001,\n        max_iter=1000,\n        random_state=42,\n        early_stopping=True,        # Stop if validation score doesn't improve\n        validation_fraction=0.1,\n        n_iter_no_change=20\n    )\n    \n    mlp_pipe = Pipeline([\n        (\"preprocess\", preprocess_pipe),\n        (\"model\", mlp)\n    ])\n    print(\"✅ Pipeline created\")\n\n    # --- 4. K-Fold Training ---\n    print(\"=\"*70)\n    print(\"CROSS-VALIDATION TRAINING\")\n    print(\"=\"*70)\n\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    fold_rmse_log, fold_rmse_orig = [], []\n    test_pred_log = np.zeros(len(X_test))\n    \n    for fold, (tr_idx, val_idx) in enumerate(kf.split(X), 1):\n        print(f\"📦 Fold {fold}/5 training...\", end=\" \")\n        X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n        y_tr, y_val = y[tr_idx], y[val_idx]\n        \n        mlp_pipe.fit(X_tr, y_tr)\n        \n        # Validation Prediction\n        y_val_pred_log = mlp_pipe.predict(X_val)\n        rmse_log = np.sqrt(mean_squared_error(y_val, y_val_pred_log))\n        \n        # Inverse transform for reporting \"Original\" RMSE\n        y_val_pred = np.expm1(y_val_pred_log)\n        y_val_true = np.expm1(y_val)\n        rmse_orig = np.sqrt(mean_squared_error(y_val_true, y_val_pred))\n        \n        fold_rmse_log.append(rmse_log)\n        fold_rmse_orig.append(rmse_orig)\n        print(f\"Log RMSE={rmse_log:.4f} | Orig RMSE={rmse_orig:.4f}\")\n        \n        # Test Prediction (Accumulate average)\n        test_pred_log += mlp_pipe.predict(X_test) / kf.n_splits\n\n    print(\"\\n=== 5-Fold Summary ===\")\n    print(f\"Mean Log RMSE={np.mean(fold_rmse_log):.4f} ± {np.std(fold_rmse_log):.4f}\")\n    print(f\"Mean Orig RMSE={np.mean(fold_rmse_orig):.4f} ± {np.std(fold_rmse_orig):.4f}\")\n\n    # --- 5. Post-Processing & Submission ---\n    print(\"=\"*70)\n    print(\"GENERATING PREDICTIONS\")\n    print(\"=\"*70)\n\n    # A. Inverse Log Transform\n    test_pred = np.expm1(test_pred_log)\n    \n    # B. Clip to valid range [0, 10]\n    test_pred_clipped = np.clip(test_pred, 0, 10)\n    \n    # C. Floor the values (Discrete integers)\n    test_pred_floored = np.floor(test_pred_clipped)\n    \n    # D. Final Safety Clip\n    final_predictions = np.clip(test_pred_floored, 0, 10)\n\n    # Create Submission DataFrame\n    if 'test_df' in globals():\n        submission_df = pd.DataFrame({\n            'ID': test_df.index + 1,\n            'score': final_predictions\n        })\n    else:\n        # Fallback if test_df isn't loaded\n        submission_df = pd.DataFrame({\n            'ID': range(1, len(final_predictions) + 1),\n            'score': final_predictions\n        })\n    \n    # Ensure float format\n    submission_df['score'] = submission_df['score'].astype(float)\n    \n    # Save\n    filename = \"submission_mlp_pca_logtarget.csv\"\n    submission_df.to_csv(filename, index=False)\n    \n    # --- 6. Final Analysis ---\n    print(f\"{filename} created successfully!\")\n    print(\"\\n--- Head ---\")\n    print(submission_df.head())\n    \n    print(\"\\n--- Prediction Value Counts ---\")\n    print(submission_df['score'].value_counts().sort_index())\n    \n    print(\"\\n--- Mean Score ---\")\n    print(f\"Mean: {submission_df['score'].mean():.4f}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T15:32:03.443123Z","iopub.execute_input":"2025-11-19T15:32:03.443409Z","iopub.status.idle":"2025-11-19T15:40:19.725209Z","shell.execute_reply.started":"2025-11-19T15:32:03.443390Z","shell.execute_reply":"2025-11-19T15:40:19.724494Z"}},"outputs":[{"name":"stdout","text":"--- Starting Strategy: Log-Target MLP with PCA (Sklearn) ---\nLoading .npy files...\nFeature Matrix Shape: (7798, 1792)\nTarget range (raw): 0.00 - 10.00\nTarget range (log): 0.0000 - 2.3979\n🏗️  Step 3: Building model pipeline...\n✅ Pipeline created\n======================================================================\nCROSS-VALIDATION TRAINING\n======================================================================\n📦 Fold 1/5 training... Log RMSE=0.1108 | Orig RMSE=0.8444\n📦 Fold 2/5 training... Log RMSE=0.1308 | Orig RMSE=0.9469\n📦 Fold 3/5 training... Log RMSE=0.1366 | Orig RMSE=0.9519\n📦 Fold 4/5 training... Log RMSE=0.1347 | Orig RMSE=0.8493\n📦 Fold 5/5 training... Log RMSE=0.1605 | Orig RMSE=1.0515\n\n=== 5-Fold Summary ===\nMean Log RMSE=0.1347 ± 0.0158\nMean Orig RMSE=0.9288 ± 0.0766\n======================================================================\nGENERATING PREDICTIONS\n======================================================================\nsubmission_mlp_pca_logtarget.csv created successfully!\n\n--- Head ---\n   ID  score\n0   1    9.0\n1   2    9.0\n2   3    8.0\n3   4    9.0\n4   5    3.0\n\n--- Prediction Value Counts ---\nscore\n0.0        1\n1.0        4\n2.0        6\n3.0       30\n4.0       42\n5.0       58\n6.0      126\n7.0      348\n8.0     1492\n9.0     1483\n10.0      48\nName: count, dtype: int64\n\n--- Mean Score ---\nMean: 8.1141\n","output_type":"stream"}],"execution_count":88}]}